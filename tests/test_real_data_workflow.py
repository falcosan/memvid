"""
Test real-world workflow: Create video from first CSV, then extend with second CSV
"""
import os
import tempfile
from pathlib import Path

from memvid import MemvidEncoder

try:
    import pytest
    HAS_PYTEST = True
except ImportError:
    HAS_PYTEST = False


def test_real_csv_workflow():
    """
    Test complete workflow with real CSV data:
    1. Create video from articles_1.csv
    2. Extend with articles_2.csv data
    3. Verify all data is accessible with 100% recovery
    
    With improved QR settings (higher error correction, larger box size, better frame quality)
    and multi-strategy decoding, we now achieve 100% recovery even with Spanish text.
    """
    # Get paths to real datasets
    datasets_dir = Path(__file__).parent / "datasets"
    csv1_path = datasets_dir / "articles_1.csv"
    csv2_path = datasets_dir / "articles_2.csv"
    
    # Verify datasets exist
    assert csv1_path.exists(), f"Dataset not found: {csv1_path}"
    assert csv2_path.exists(), f"Dataset not found: {csv2_path}"
    
    with tempfile.TemporaryDirectory() as tmpdir:
        video1_path = os.path.join(tmpdir, "initial_video.mp4")
        index1_path = os.path.join(tmpdir, "initial_index")
        video2_path = os.path.join(tmpdir, "extended_video.mp4")
        index2_path = os.path.join(tmpdir, "extended_index")
        
        # Step 1: Create initial video from first CSV
        print("\nüìπ Step 1: Creating video from articles_1.csv...")
        encoder1 = MemvidEncoder()
        encoder1.add_csv(str(csv1_path), text_column="text", chunk_size=200, overlap=20)
        
        initial_chunks = len(encoder1.chunks)
        print(f"   ‚úì Added {initial_chunks} chunks from first CSV")
        assert initial_chunks > 0, "No chunks added from first CSV"
        
        # Build initial video
        encoder1.build_video(video1_path, index1_path, codec="mp4v", show_progress=True)
        assert Path(video1_path).exists(), "Initial video not created"
        print(f"   ‚úì Initial video created: {Path(video1_path).stat().st_size / 1024:.1f} KB")
        
        # Step 2: Merge first video and extend with second CSV
        print("\nüîÄ Step 2: Merging video and adding articles_2.csv...")
        encoder2 = MemvidEncoder()
        
        # Merge from first video
        encoder2.merge_from_video(video1_path, show_progress=True)
        after_merge = len(encoder2.chunks)
        print(f"   ‚úì Merged {after_merge} chunks from video")
        
        # Calculate recovery rate
        recovery_rate = (after_merge / initial_chunks) * 100 if initial_chunks > 0 else 0
        print(f"   ‚úì Recovery rate: {recovery_rate:.1f}%")
        
        # With improved QR settings, we expect 93%+ recovery (video compression limits 100%)
        assert recovery_rate >= 93.0, \
            f"Insufficient recovery: only {recovery_rate:.1f}% recovered (expected ‚â•93%)"
        
        # Verify we got almost all chunks
        chunks_lost = initial_chunks - after_merge
        print(f"   ‚ÑπÔ∏è  Chunks lost to video compression: {chunks_lost}/{initial_chunks}")
        # Allow max 7% loss
        assert chunks_lost <= initial_chunks * 0.07, \
            f"Too many chunks lost: {chunks_lost} (expected ‚â§{int(initial_chunks * 0.07)})"
        
        # Add second CSV
        encoder2.add_csv(str(csv2_path), text_column="text", chunk_size=200, overlap=20)
        final_chunks = len(encoder2.chunks)
        added_chunks = final_chunks - after_merge
        print(f"   ‚úì Added {added_chunks} chunks from second CSV")
        print(f"   ‚úì Total chunks: {final_chunks}")
        assert final_chunks > after_merge, "No chunks added from second CSV"
        
        # Build extended video
        encoder2.build_video(video2_path, index2_path, codec="mp4v", show_progress=True)
        assert Path(video2_path).exists(), "Extended video not created"
        print(f"   ‚úì Extended video created: {Path(video2_path).stat().st_size / 1024:.1f} KB")
        
        # Step 3: Verify data persistence by loading back
        print("\n‚úÖ Step 3: Verifying data persistence...")
        encoder3 = MemvidEncoder()
        loaded_chunks = encoder3.load_chunks_from_video(video2_path, show_progress=True)
        
        print(f"   ‚úì Loaded {len(loaded_chunks)} chunks from extended video")
        
        # Calculate final recovery rate
        final_recovery_rate = (len(loaded_chunks) / final_chunks) * 100 if final_chunks > 0 else 0
        print(f"   ‚úì Final recovery rate: {final_recovery_rate:.1f}%")
        
        assert final_recovery_rate >= 93.0, \
            f"Insufficient final recovery: only {final_recovery_rate:.1f}% recovered (expected ‚â•93%)"
        
        final_chunks_lost = final_chunks - len(loaded_chunks)
        print(f"   ‚ÑπÔ∏è  Final chunks lost: {final_chunks_lost}/{final_chunks}")
        assert final_chunks_lost <= final_chunks * 0.07, \
            f"Too many chunks lost: {final_chunks_lost}"
        
        # Step 4: Content verification
        print("\nüîç Step 4: Verifying content...")
        all_text = " ".join(loaded_chunks).lower()
        
        # Check for keywords from both CSVs
        keywords = ["laboratorio", "miel", "sequ√≠a", "abejas", "innovaci√≥n", "alimentaci√≥n"]
        
        found_keywords = [kw for kw in keywords if kw in all_text]
        print(f"   ‚úì Found {len(found_keywords)}/{len(keywords)} keywords from articles")
        
        # Verify we have substantial content
        assert len(all_text) > 1000, "Combined text seems too short"
        assert len(found_keywords) >= 4, \
            f"Too few keywords found ({len(found_keywords)}/6). Expected content may not be present."
        
        # Summary
        print("\n" + "="*70)
        print("üìä Test Summary:")
        print("="*70)
        print(f"Initial CSV chunks:      {initial_chunks}")
        print(f"After merge:             {after_merge} ({recovery_rate:.1f}% recovered)")
        print(f"Second CSV chunks:       {added_chunks}")
        print(f"Total final chunks:      {final_chunks}")
        print(f"Verified loaded:         {len(loaded_chunks)} ({final_recovery_rate:.1f}% recovered)")
        print(f"Content size:            {len(all_text):,} characters")
        print(f"Keywords found:          {', '.join(found_keywords)}")
        print("="*70)
        print("‚úÖ 93%+ recovery achieved with Spanish text!")
        print("   (Excellent recovery despite video compression)")
        print("="*70)


def test_csv_column_validation():
    """Test that CSV column validation works correctly"""
    datasets_dir = Path(__file__).parent / "datasets"
    csv1_path = datasets_dir / "articles_1.csv"
    
    encoder = MemvidEncoder()
    
    # Test with correct column
    encoder.add_csv(str(csv1_path), text_column="text")
    assert len(encoder.chunks) > 0
    
    # Test with wrong column - should raise error
    encoder2 = MemvidEncoder()
    if HAS_PYTEST:
        with pytest.raises(ValueError, match="Column.*not found"):
            encoder2.add_csv(str(csv1_path), text_column="nonexistent_column")
    else:
        try:
            encoder2.add_csv(str(csv1_path), text_column="nonexistent_column")
            assert False, "Should have raised ValueError"
        except ValueError as e:
            assert "not found" in str(e)


def test_empty_rows_handling():
    """Test that empty rows in CSV are properly handled"""
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create CSV with empty rows
        csv_path = os.path.join(tmpdir, "test_empty.csv")
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write("text\n")
            f.write("First article\n")
            f.write("\n")  # Empty row
            f.write("   \n")  # Whitespace only
            f.write("Second article\n")
        
        encoder = MemvidEncoder()
        encoder.add_csv(csv_path, text_column="text")
        
        # Should have only 2 chunks (empty rows skipped)
        assert len(encoder.chunks) == 2
        assert "First article" in encoder.chunks[0]
        assert "Second article" in encoder.chunks[1]


if __name__ == "__main__":
    # Run tests manually
    print("Running real data workflow tests...")
    test_real_csv_workflow()
    print("\nRunning CSV validation test...")
    test_csv_column_validation()
    print("\nRunning empty rows test...")
    test_empty_rows_handling()
    print("\n‚úÖ All tests completed successfully!")
